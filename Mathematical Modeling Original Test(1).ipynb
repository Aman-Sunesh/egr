{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819a4c99-271f-4272-8dd7-e9981ad3533b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nolds in /home/lab-user/anaconda3/lib/python3.12/site-packages (0.6.1)\n",
      "Requirement already satisfied: numpy<3.0,>1.0 in /home/lab-user/anaconda3/lib/python3.12/site-packages (from nolds) (1.26.4)\n",
      "Requirement already satisfied: future in /home/lab-user/anaconda3/lib/python3.12/site-packages (from nolds) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /home/lab-user/anaconda3/lib/python3.12/site-packages (from nolds) (75.1.0)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import io\n",
    "import numpy as np\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "!pip install nolds\n",
    "import nolds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ce57c-0f1e-4de6-8554-58ada832a459",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29157292-ed8a-498a-bc5d-b80b22abd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lorentz_System(state, sigma, rho, beta):\n",
    "    x, y, z = state\n",
    "    x_dot = sigma*(y-x)\n",
    "    y_dot = x*(rho-z) - y\n",
    "    z_dot = x*y - beta*z\n",
    "\n",
    "    return np.array([x_dot, y_dot, z_dot])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485a94c-c740-460f-9c63-774757ec48d2",
   "metadata": {},
   "source": [
    "### Runga Kutta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccaae51-dfa1-4137-b25c-1ed2350c4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk4(state, dt, sigma, rho, beta):\n",
    "    k1 = Lorentz_System(state, sigma, rho, beta)\n",
    "    k2 = Lorentz_System(state + 0.5 * dt * k1, sigma, rho, beta)\n",
    "    k3 = Lorentz_System(state + 0.5 * dt * k2, sigma, rho, beta)\n",
    "    k4 = Lorentz_System(state + dt * k3, sigma, rho, beta)\n",
    "\n",
    "    return state + (dt*((k1 + 2*k2 + 2*k3 + k4)/6.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b34e2-759f-4906-bfb7-6e98d4aa2690",
   "metadata": {},
   "source": [
    "Now, let's visualize the Lorenz attractor using N = 500,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d226c-0a43-46ba-bd29-f7cb851d45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "N = 500000\n",
    "sigma, rho, beta = 10.0, 28.0, 8.0/3.0\n",
    "x0, y0, z0 = 1.0, 1.0, 1.0\n",
    "state = np.array([x0, y0, z0])\n",
    "\n",
    "trajectory = np.empty((N+1, 3))\n",
    "trajectory[0] = state\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    state = rk4(state, dt, sigma, rho, beta)\n",
    "    trajectory[i] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bd1fa-0611-45df-a112-9d9545101f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample trajectory to plot fewer points\n",
    "thin = trajectory[::1]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "# 3D\n",
    "ax0 = fig.add_subplot(1, 4, 1, projection='3d')\n",
    "ax0.plot(thin[:, 0], thin[:, 1], thin[:, 2], lw=0.5)\n",
    "ax0.set_xlabel('x(t)'); ax0.set_ylabel('y(t)'); ax0.set_zlabel('z(t)')\n",
    "ax0.set_title('3D View')\n",
    "ax0.set_box_aspect((1,1,1))  # equal aspect in 3D\n",
    "\n",
    "# XY\n",
    "ax1 = fig.add_subplot(1, 4, 2)\n",
    "ax1.plot(thin[:, 0], thin[:, 1], lw=0.5)\n",
    "ax1.set_xlabel('x(t)'); ax1.set_ylabel('y(t)')\n",
    "ax1.set_title('XY Projection')\n",
    "ax1.set_aspect('equal', 'box')\n",
    "\n",
    "# YZ\n",
    "ax2 = fig.add_subplot(1, 4, 3)\n",
    "ax2.plot(thin[:, 1], thin[:, 2], lw=0.5)\n",
    "ax2.set_xlabel('y(t)'); ax2.set_ylabel('z(t)')\n",
    "ax2.set_title('YZ Projection')\n",
    "ax2.set_aspect('equal', 'box')\n",
    "\n",
    "# XZ\n",
    "ax3 = fig.add_subplot(1, 4, 4)\n",
    "ax3.plot(thin[:, 0], thin[:, 2], lw=0.5)\n",
    "ax3.set_xlabel('x(t)'); ax3.set_ylabel('z(t)')\n",
    "ax3.set_title('XZ Projection')\n",
    "ax3.set_aspect('equal', 'box')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5cc19-b51e-4cc4-9637-af83798744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trajectory[-1000::5,0], 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32c2e0-6895-4f02-9fcc-0d9c1cc0fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First point of full trajectory:\", trajectory[0])\n",
    "print(\"Second point of full trajectory:\", trajectory[1])\n",
    "print(\"Third point of full trajectory:\", trajectory[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b5244-d5a0-4c0b-ae8b-8d525f61b535",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bd97c-52a1-4790-b508-0c58934af6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(trajectory, window_size, horizon = 1):\n",
    "    # horizon is the number of steps ahead to predict\n",
    "\n",
    "    N = trajectory.shape[0]\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(N - window_size - horizon + 1):\n",
    "        X.append((trajectory[i : i + window_size]).flatten())\n",
    "        y.append((trajectory[i + window_size : i + window_size + horizon]).flatten())\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d46b1-6e3f-47bf-8a62-abd12e839057",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b56d93-70be-4293-877f-431292f8d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size = 64, hidden_size = 30, output_size = 10, learning_rate = 1e-3, epochs = 1000):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Weights & bias initialization with small random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01  \n",
    "        self.b1 = np.zeros((1, hidden_size))                       \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01 \n",
    "        self.b2 = np.zeros((1, output_size))                       #\n",
    "\n",
    "    # ReLU activation function\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Derivative of the ReLU function\n",
    "    def ReLU_deriv(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    # Linear Activation Function\n",
    "    def Linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def Linear_deriv(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    # Forward propagation: computes the activations for the hidden and output layers\n",
    "    def forward_propagation(self, x):\n",
    "        # Hidden layer\n",
    "        self.z1 = x @ self.W1 + self.b1\n",
    "        self.a1 = self.ReLU(self.z1)\n",
    "    \n",
    "        # Output layer\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.Linear(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "\n",
    "    # Compute the cost using Mean Squared Error\n",
    "    def compute_cost(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    # Back propagation: computes gradients for updating weights and biases\n",
    "    def back_propagation(self, x, y):       \n",
    "        m = y.shape[0]\n",
    "    \n",
    "        # Compute error at output layer\n",
    "        error2 = 2 * (self.a2 - y) * self.Linear_deriv(self.z2)\n",
    "\n",
    "        # Compute gradients for weights and biases for the output layer\n",
    "        grad_W2 = (self.a1.T @ error2) / m\n",
    "        grad_b2 = np.sum(error2, axis = 0, keepdims = True) / m\n",
    "\n",
    "        # Backpropagate the error to the hidden layer\n",
    "        error1 = (error2 @ self.W2.T) * self.ReLU_deriv(self.z1)\n",
    "\n",
    "        # Compute gradients for weights and biases for the hidden layer\n",
    "        grad_W1 = (x.T @ error1) / m\n",
    "        grad_b1 = np.sum(error1, axis = 0, keepdims = True) / m\n",
    "\n",
    "        return grad_W1, grad_b1, grad_W2, grad_b2\n",
    "\n",
    "    # Update the network parameters (weights and biases) using the computed gradients\n",
    "    def update_parameters(self, grad_W1, grad_b1, grad_W2, grad_b2):\n",
    "        self.W2 = self.W2 - (self.learning_rate * grad_W2)\n",
    "        self.b2 = self.b2 - (self.learning_rate * grad_b2)\n",
    "        self.W1 = self.W1 - (self.learning_rate * grad_W1)\n",
    "        self.b1 = self.b1 - (self.learning_rate * grad_b1)\n",
    "\n",
    "    # Train the neural network by iterating over epochs\n",
    "    def train(self, x, y, optimizer='sgd'):\n",
    "        optimizer = optimizer.lower()\n",
    "        \n",
    "        if optimizer not in ('sgd', 'adam'):\n",
    "            raise ValueError(\"Optimizer must be 'sgd' or 'adam'\")\n",
    "\n",
    "        # Adam hyperparameters\n",
    "        if optimizer == 'adam':\n",
    "            beta1, beta2 = 0.9, 0.999\n",
    "            eps = 1e-8\n",
    "            # Initialize 1st & 2nd moment vectors\n",
    "            m_W1 = np.zeros_like(self.W1); v_W1 = np.zeros_like(self.W1)\n",
    "            m_b1 = np.zeros_like(self.b1); v_b1 = np.zeros_like(self.b1)\n",
    "            m_W2 = np.zeros_like(self.W2); v_W2 = np.zeros_like(self.W2)\n",
    "            m_b2 = np.zeros_like(self.b2); v_b2 = np.zeros_like(self.b2)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            # Perform forward propagation to get outputs\n",
    "            Y_pred = self.forward_propagation(x)\n",
    "\n",
    "            # Compute gradients via back propagation\n",
    "            grad_W1, grad_b1, grad_W2, grad_b2 = self.back_propagation(x, y)\n",
    "\n",
    "            if optimizer == 'sgd':\n",
    "                # Update network parameters with the gradients using Standard SGD\n",
    "                self.update_parameters(grad_W1, grad_b1, grad_W2, grad_b2)\n",
    "\n",
    "            else:\n",
    "                # Adam updates\n",
    "                m_W1 = beta1 * m_W1 + (1 - beta1) * grad_W1\n",
    "                v_W1 = beta2 * v_W1 + (1 - beta2) * (grad_W1 ** 2)\n",
    "                m_b1 = beta1 * m_b1 + (1 - beta1) * grad_b1\n",
    "                v_b1 = beta2 * v_b1 + (1 - beta2) * (grad_b1 ** 2)\n",
    "                m_W2 = beta1 * m_W2 + (1 - beta1) * grad_W2\n",
    "                v_W2 = beta2 * v_W2 + (1 - beta2) * (grad_W2 ** 2)\n",
    "                m_b2 = beta1 * m_b2 + (1 - beta1) * grad_b2\n",
    "                v_b2 = beta2 * v_b2 + (1 - beta2) * (grad_b2 ** 2)\n",
    "\n",
    "                # Bias-corrected moments\n",
    "                mW1_hat = m_W1 / (1 - beta1 ** epoch)\n",
    "                vW1_hat = v_W1 / (1 - beta2 ** epoch)\n",
    "                mb1_hat = m_b1 / (1 - beta1 ** epoch)\n",
    "                vb1_hat = v_b1 / (1 - beta2 ** epoch)\n",
    "                mW2_hat = m_W2 / (1 - beta1 ** epoch)\n",
    "                vW2_hat = v_W2 / (1 - beta2 ** epoch)\n",
    "                mb2_hat = m_b2 / (1 - beta1 ** epoch)\n",
    "                vb2_hat = v_b2 / (1 - beta2 ** epoch)\n",
    "\n",
    "                # Update parameters\n",
    "                self.W1 -= self.learning_rate * mW1_hat / (np.sqrt(vW1_hat) + eps)\n",
    "                self.b1 -= self.learning_rate * mb1_hat / (np.sqrt(vb1_hat) + eps)\n",
    "                self.W2 -= self.learning_rate * mW2_hat / (np.sqrt(vW2_hat) + eps)\n",
    "                self.b2 -= self.learning_rate * mb2_hat / (np.sqrt(vb2_hat) + eps)\n",
    "\n",
    "            # Every 100 epochs, compute and print the cost for monitoring convergence\n",
    "            if (epoch%100 == 0):\n",
    "                cost = self.compute_cost(Y_pred, y)\n",
    "                print(f\"Epoch {epoch}, Cost: {cost:.6f}, Optimizer: {optimizer.upper()}\")\n",
    "\n",
    "    # Predict class labels for input data\n",
    "    def predict(self, x):\n",
    "        return self.forward_propagation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42b4c2-96d9-46f9-8c21-fd0d4c0d5495",
   "metadata": {},
   "source": [
    "# Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98ebc1-d511-444f-ac32-2e5a023511bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "X1, y1 = make_windows(trajectory, 20, horizon = 1)\n",
    "n1    = X1.shape[0]\n",
    "n1_train = int(0.7 * n1)\n",
    "\n",
    "X1_train, y1_train = X1[:n1_train], y1[:n1_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4ca97-b4e5-40e6-ab59-807b3903db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmu  = X1_train.mean(0, keepdims=True)\n",
    "Xstd = X1_train.std(0, keepdims=True) + eps\n",
    "ymu  = y1_train.mean(0, keepdims=True)\n",
    "ystd = y1_train.std(0, keepdims=True) + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195bf02-42ef-480c-9f34-c8b3e2ac7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'window_size': 20, 'hidden_size': 100, 'learning_rate': 0.01, 'batch_size': 64, 'optimizer': 'adam', 'epochs': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ebbc3-2c51-45fe-b4ef-5cf4094b0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "horizons    = [1, 2, 3, 5, 10]   \n",
    "\n",
    "results = {}\n",
    "for h in horizons:\n",
    "    Xh, yh = make_windows(trajectory, window_size, horizon=h)\n",
    "\n",
    "    n = Xh.shape[0]\n",
    "    n_train = int(0.70*n)\n",
    "    n_val   = int(0.15*n)\n",
    "\n",
    "    Xh_train, yh_train = Xh[:n_train], yh[:n_train]\n",
    "    Xh_val,   yh_val   = Xh[n_train:n_train+n_val], yh[n_train:n_train+n_val]\n",
    "    Xh_test,  yh_test  = Xh[n_train+n_val:], yh[n_train+n_val:]\n",
    "\n",
    "    # normalize exactly as before\n",
    "    Xh_train = (Xh_train - Xmu) / Xstd\n",
    "    Xh_val   = (Xh_val   - Xmu) / Xstd\n",
    "    Xh_test  = (Xh_test  - Xmu) / Xstd\n",
    "\n",
    "    ymu  = yh_train.mean(0, keepdims=True)\n",
    "    ystd = yh_train.std(0, keepdims=True) + eps\n",
    "\n",
    "    yh_train = (yh_train - ymu) / ystd\n",
    "    yh_val   = (yh_val   - ymu) / ystd\n",
    "    yh_test  = (yh_test  - ymu) / ystd\n",
    "\n",
    "    model_h = NeuralNetwork(\n",
    "        input_size    = window_size * 3,\n",
    "        hidden_size   = best_params['hidden_size'],\n",
    "        output_size   = 3 * h,\n",
    "        learning_rate = best_params['learning_rate'],\n",
    "        epochs        = best_params['epochs'],\n",
    "    )\n",
    "    \n",
    "    model_h.train(Xh_train, yh_train, optimizer=best_params['optimizer'])\n",
    "\n",
    "    yhat_h = model_h.predict(Xh_test) * ystd + ymu\n",
    "    rmse_h = np.sqrt(np.mean((yhat_h - yh_test * ystd - ymu)**2))\n",
    "    results[h] = rmse_h\n",
    "\n",
    "for h, rmse in results.items():\n",
    "    print(f\"Horizon {h:2d}: direct RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d32f15-efc6-4945-a7a2-fdf9ff20b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = 10\n",
    "\n",
    "model_direct = direct_models[h]   \n",
    "\n",
    "L0 = n_train + n_val\n",
    "window = trajectory[L0 : L0 + window_size].copy()  \n",
    "\n",
    "x_norm = (window.flatten()[None, :] - Xmu) / Xstd      # shape (1, window_size*3)\n",
    "yhat   = model_direct.predict(x_norm)                  # shape (1, 3*h)\n",
    "yhat   = (yhat * ystd + ymu).reshape(h, 3)            \n",
    "\n",
    "true_future = trajectory[L0 + window_size : L0 + window_size + h, 0]  # x‐coordinate\n",
    "forecast_x  = yhat[:, 0]  # predicted x\n",
    "\n",
    "true_past = trajectory[L0 : L0 + window_size, 0]\n",
    "t_past    = np.arange(-window_size, 0)\n",
    "t_fut     = np.arange(0, h)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(t_past, true_past,   'o-', label=f\"True past ({window_size})\")\n",
    "plt.plot(t_fut,    true_future, '-',  label=\"True future\", alpha=0.6)\n",
    "plt.plot(t_fut,    forecast_x, '--', label=f\"Direct forecast (h={h})\", alpha=0.8)\n",
    "plt.axvline(0, color='k', linestyle='--', lw=1)\n",
    "plt.xlabel(\"Relative time step\")\n",
    "plt.ylabel(\"x coordinate\")\n",
    "plt.title(f\"Direct‐method Forecast vs. True, horizon={h}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(t_fut, true_future, label=\"True future\", alpha=0.6)\n",
    "plt.plot(t_fut, forecast_x, '--', label=f\"Forecast (h={h})\", alpha=0.8)\n",
    "plt.xlabel(\"Relative time step\")\n",
    "plt.ylabel(\"x coordinate\")\n",
    "plt.title(f\"Direct‐method Forecast vs. True continuation (h={h})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed39c7-6cac-4f70-ba5e-93e3d3b7d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "L0 = n_train + n_val\n",
    "L0 + window_size + h <= trajectory.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439deca-0c35-4708-98a8-d4b6e780aa6f",
   "metadata": {},
   "source": [
    "### Splitting into Training, Validation, and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa44d62-b335-4bd0-94e2-4ef7bdbc10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = X.shape[0]\n",
    "# n_train = int(0.70*n)\n",
    "# n_val   = int(0.15*n)\n",
    "\n",
    "# X_train, y_train = X[:n_train], y[:n_train]\n",
    "# X_val,   y_val   = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
    "# X_test,  y_test  = X[n_train+n_val:], y[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572cb4f-598c-4e79-9cfc-6df0d44c819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Total points: {N}\")\n",
    "# print(f\"  Training : {X_train.shape[0]} samples\")\n",
    "# print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "# print(f\"  Testing  : {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733de092-ce82-4b5d-9495-d933de5afa57",
   "metadata": {},
   "source": [
    "### Normalizing the Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfdd08e-78a1-4201-be55-633c3abf6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = 1e-8   # Added to standard deviation values to avoid division by zero error\n",
    "# Xmu,  Xstd  = X_train.mean(0, keepdims=True), X_train.std(0, keepdims=True) + eps\n",
    "# ymu,  ystd  = y_train.mean(0, keepdims=True), y_train.std(0, keepdims=True) + eps\n",
    "\n",
    "# X_train = (X_train - Xmu)/Xstd\n",
    "# X_val   = (X_val   - Xmu)/Xstd\n",
    "# X_test  = (X_test  - Xmu)/Xstd\n",
    "\n",
    "# y_train = (y_train - ymu)/ystd\n",
    "# y_val   = (y_val   - ymu)/ystd\n",
    "# y_test  = (y_test  - ymu)/ystd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e8990-63dc-4858-9e85-5df11c114cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(\n",
    "#     input_size    = window_size * 3,\n",
    "#     hidden_size   = 30,\n",
    "#     output_size   = horizon * 3,\n",
    "#     learning_rate = 1e-3,\n",
    "#     epochs        = 1000 \n",
    "# )\n",
    "\n",
    "# model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc9ed3-f896-4cff-b629-978b8c1b6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat_val = model.predict(X_val)*ystd + ymu\n",
    "# mse_val   = np.mean((y_hat_val - (y_val*ystd + ymu))**2)\n",
    "# print(\"Validation MSE:\", mse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389be217-9c02-4c29-9ef8-661e18658c4b",
   "metadata": {},
   "source": [
    "Typical x, y, z values on the attractor are roughly in the range [-20, 40]. RMSE = sqrt(MSE) = 2.55. So, on average, each coordinate is off by ≈ 2.5 units one time-step (Δt=0.01) into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d3961-e872-4ce9-9320-e5ddaf45d94b",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23f5c7-43f7-45da-a117-48c26a343420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'window_size':  [10, 20, 50],\n",
    "#     'hidden_size':  [30,  50, 100],\n",
    "#     'learning_rate':[1e-3,1e-2],\n",
    "#     'batch_size':   [32,  64],\n",
    "#     'optimizer':    ['sgd','adam'],\n",
    "#     'epochs':       [100, 500, 1000]\n",
    "# }\n",
    "\n",
    "# best_mse    = float('inf')\n",
    "# best_params = None\n",
    "# best_model  = None\n",
    "# best_hist   = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d863a07-aa42-46ea-b128-86f30346ce74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for combo in itertools.product(*param_grid.values()):\n",
    "#     params = dict(zip(param_grid.keys(), combo))\n",
    "#     X_train_w, y_train_w = make_windows(trajectory[:n_train], params['window_size'])\n",
    "#     X_val_w, y_val_w   = make_windows(trajectory[n_train:n_train+n_val], params['window_size'])\n",
    "\n",
    "#     model = NeuralNetwork(\n",
    "#         input_size    = params['window_size'] * 3,\n",
    "#         hidden_size   = params['hidden_size'],\n",
    "#         output_size   = 3,                     \n",
    "#         learning_rate = params['learning_rate'],\n",
    "#         epochs        = params['epochs']\n",
    "#     )\n",
    "    \n",
    "#     history = model.train(\n",
    "#         X_train_w, y_train_w,\n",
    "#         optimizer = params['optimizer'],\n",
    "#     )\n",
    "\n",
    "#     val_pred = model.predict(X_val_w)\n",
    "#     val_mse  = np.mean((val_pred - y_val_w)**2)\n",
    "\n",
    "#     print(f\"Params={params}  →  Val MSE={val_mse:.4e}\")\n",
    "    \n",
    "#     if val_mse < best_mse:\n",
    "#         best_mse    = val_mse\n",
    "#         best_params = params\n",
    "#         best_model  = deepcopy(model)\n",
    "#         best_hist   = history\n",
    "\n",
    "# print(\"\\nBest params:\", best_params)\n",
    "# print(\"Best validation MSE:\", best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58acb99-78e7-4f5e-8039-76cbe58dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {'window_size': 20, 'hidden_size': 100, 'learning_rate': 0.01, 'batch_size': 64, 'optimizer': 'adam', 'epochs': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a12b7-f54b-4f19-be33-816d58bcdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nBest params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148051c-5f38-4a3d-ad76-8e7db78431e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_trainval, y_trainval = make_windows(trajectory[:n_train+n_val], best_params['window_size'])\n",
    "# X_test_w,   y_test_w   = make_windows(trajectory[n_train+n_val:], best_params['window_size'])\n",
    "\n",
    "# X_trainval = (X_trainval - Xmu) / Xstd\n",
    "# y_trainval = (y_trainval - ymu) / ystd\n",
    "\n",
    "# X_test_w = (X_test_w - Xmu) / Xstd\n",
    "# y_test_w = (y_test_w - ymu) / ystd\n",
    "\n",
    "# final_model = NeuralNetwork(\n",
    "#     input_size  = best_params['window_size'] * 3,\n",
    "#     hidden_size = best_params['hidden_size'],\n",
    "#     learning_rate = best_params['learning_rate'],\n",
    "#     epochs      = best_params['epochs'],\n",
    "#     output_size = 3\n",
    "# )\n",
    "\n",
    "# final_hist = final_model.train(\n",
    "#     X_trainval, y_trainval,\n",
    "#     optimizer   = best_params['optimizer']    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076225f-4f08-4166-95e4-b60d95d301bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = final_model.predict(X_test_w)\n",
    "# test_mse  = np.mean((test_pred - y_test_w)**2)\n",
    "# print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e14f6-7ab8-4e32-b9ba-92df48d25452",
   "metadata": {},
   "source": [
    "RMSE = 0.074. So, on average, each coorainet is off by ~0.07 units just one time-step (Δt = 0.01) into the future. It captures the immediate local dynamics of a chaotic system with less than a tenth of a unit error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8b508-aa85-4bc1-a9de-7613d3aa1c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Capture the printed training output into a string buffer\n",
    "# buffer = io.StringIO()\n",
    "# old_stdout = sys.stdout\n",
    "# sys.stdout = buffer\n",
    "\n",
    "# # Retrain on your best model (this will still print, but now into `buffer`)\n",
    "# final_model.train(X_trainval, y_trainval, optimizer=best_params['optimizer'])\n",
    "\n",
    "# sys.stdout = old_stdout\n",
    "\n",
    "# # Extract epoch & cost from the captured log\n",
    "# log_lines = buffer.getvalue().splitlines()\n",
    "# epochs, losses = [], []\n",
    "# for line in log_lines:\n",
    "#     m = re.match(r\"Epoch\\s+(\\d+), Cost:\\s*([\\d\\.eE+-]+)\", line)\n",
    "#     if m:\n",
    "#         epochs.append(int(m.group(1)))\n",
    "#         losses.append(float(m.group(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35587785-1201-4fbd-b467-d135a7accf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,4))\n",
    "# plt.plot(epochs, losses, marker='o', label='Train Loss')\n",
    "# plt.title(\"Train Loss (best model retrained)\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1f055-8bed-45cd-b7b4-accebca81979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- parameters ----\n",
    "window_size = best_params['window_size']\n",
    "n_forecast  = 1000\n",
    "horizon     = 40\n",
    "dim         = 3\n",
    "\n",
    "# 1) Rebuild windows to locate test start\n",
    "X_all, y_all = make_windows(trajectory, window_size, horizon)\n",
    "n            = X_all.shape[0]\n",
    "n_train      = int(0.70 * n)\n",
    "n_val        = int(0.15 * n)\n",
    "test_start   = n_train + n_val   # index in X_all where X_test begins\n",
    "\n",
    "# 2) Grab the window that *ends* right before your test set\n",
    "#    In trajectory coordinates, that window starts at `test_start` and spans window_size points\n",
    "L0     = test_start\n",
    "window = trajectory[L0 : L0 + window_size].copy()   # shape (window_size, 3)\n",
    "\n",
    "# 3) Closed‐loop forecast\n",
    "forecast = np.zeros((n_forecast, dim))\n",
    "for t in range(n_forecast):\n",
    "    # normalize, predict, denormalize\n",
    "    x_norm = (window.flatten()[None, :] - Xmu) / Xstd    # shape (1, window_size*3)\n",
    "    y_norm = final_model.predict(x_norm)                 # shape (1,3)\n",
    "    y      = (y_norm * ystd + ymu).flatten()\n",
    "    forecast[t] = y\n",
    "    # slide the window\n",
    "    window = np.vstack([window[1:], y])\n",
    "\n",
    "# 4) Extract the true past & true future in the x‐coordinate\n",
    "true_past   = trajectory[L0 : L0 + window_size,        0]  # x at t = L0 … L0+window_size–1\n",
    "true_future = trajectory[L0 + window_size : L0 + window_size + n_forecast, 0]\n",
    "forecast_x  = forecast[:, 0]\n",
    "\n",
    "# time axes\n",
    "t_past = np.arange(-window_size, 0)     # [-window_size … -1]\n",
    "t_fut  = np.arange(0, n_forecast)       # [0 … n_forecast-1]\n",
    "\n",
    "# --- Plot 1: true past → true future continuity ---\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot( t_past, true_past,   'o-', label=f\"True past ({window_size})\" )\n",
    "plt.plot(    t_fut, true_future,  '-', label=\"True future\", alpha=0.6 )\n",
    "plt.axvline(0, color='k', linestyle='--', lw=1)\n",
    "plt.xlabel(\"Relative time step\")\n",
    "plt.ylabel(\"x coordinate\")\n",
    "plt.title(\"True Lorenz: past → future\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: model forecast vs true continuation ---\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(    t_fut, true_future, label=\"True future\", alpha=0.6 )\n",
    "plt.plot(    t_fut, forecast_x, '--', label=\"Forecast\",    alpha=0.8 )\n",
    "plt.xlabel(\"Relative time step\")\n",
    "plt.ylabel(\"x coordinate\")\n",
    "plt.title(\"Closed‐loop Forecast vs. True continuation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23040d5c-7fa3-4fb8-b254-537232da03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [1,2,3,4,5]\n",
    "per_step_rmse = {}\n",
    "for h in horizons:\n",
    "    err = forecast_x[h-1] - true_future[h-1]\n",
    "    per_step_rmse[h] = abs(err)  # single-point RMSE = abs(error)\n",
    "print(\"Per-step errors:\", per_step_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d46cf-0db9-430d-b3f0-3443c054316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nolds\n",
    "\n",
    "ts = trajectory[:,0]\n",
    "\n",
    "largest_le = nolds.lyap_r(ts,\n",
    "                          emb_dim=3,      # start with m=3\n",
    "                          lag=10,         # e.g. τ=10 samples = 0.1 s\n",
    "                          min_tsep=100)   # e.g. only consider neighbors ≥100 samples apart\n",
    "print(\"Estimated largest Lyapunov exponent:\", largest_le, \"bits/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be298f4-7bd7-4eaa-8563-40bd757e92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.abs(forecast_x - true_future)       # absolute error at each step\n",
    "t   = np.arange(err.size) * dt              # physical time of each forecast step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028fda8-9484-4bae-910b-198c39567eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# choose a window where err is small but growing cleanly, say the first 50 steps\n",
    "n_fit = 50\n",
    "t_fit = t[:n_fit].reshape(-1,1)\n",
    "log_err = np.log(err[:n_fit] + 1e-12)   # tiny offset so you never log(0)\n",
    "\n",
    "model = LinearRegression().fit(t_fit, log_err)\n",
    "lambda_est = model.coef_[0]            # this is your Lyapunov‐style growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51990f-4dee-45f4-b37d-d5e21e90583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 30.0         # e.g. half your attractor’s x‐span\n",
    "err0      = err[0] + 1e-12\n",
    "\n",
    "T_horizon   = (np.log(threshold/err0) / lambda_est)  # seconds\n",
    "steps_horizon = T_horizon / dt                       # in timesteps\n",
    "\n",
    "print(f\"Growth rate λ ≈ {lambda_est:.3f} s⁻¹\")\n",
    "print(f\"Forecast remains decent up to ≈ {T_horizon:.2f} s \" \n",
    "      f\"({steps_horizon:.0f} steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cc169-cb92-43e6-acdf-280717076985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
